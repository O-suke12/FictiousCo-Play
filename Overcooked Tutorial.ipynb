{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked Ai Library, including loading and evaluating agents and visualizing trajectories. Ideally we will have a Colab notebook you can interact with, but sadly Colab only supports python 3.10 kernel, and currently there are problems loading files pickled in 3.7 environment. As a compromise we created this notebook where you can see some examples of the most frequently used methods after they are executed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# 0): Training you agent\n",
    "The most convenient way to train an agent is with the [ppo_rllib_client.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/ppo/ppo_rllib_client.py) file, where you can either pass in the arguments through commandline, or you can directly modify the variables you want to change in the file. \n",
    "\n",
    "You can also start an experiment in another python script like the following, which can sometimes be more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60521ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_aware_rl.ppo.ppo_rllib_client import ex\n",
    "# For all the tunable paramters, check out ppo_rllib_client.py file\n",
    "# Note this is not what the configuration should look like for a real experiment\n",
    "config_updates = {\n",
    "    \"results_dir\": \"path/to/results\", #change this to your local directory\n",
    "    \"layout_name\": \"cramped_room\",\n",
    "    \"clip_param\": 0.2,\n",
    "    'gamma': 0.9,\n",
    "    'num_training_iters': 10, #this should usually be a lot higher\n",
    "    'num_workers': 1,\n",
    "    'num_gpus': 0,\n",
    "    \"verbose\": True,\n",
    "    'train_batch_size': 800,\n",
    "    'sgd_minibatch_size': 800,\n",
    "    'num_sgd_iter': 1,\n",
    "    \"evaluation_interval\": 2\n",
    "}\n",
    "run = ex.run(config_updates=config_updates, options={\"--loglevel\": \"ERROR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04e2536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sacred.run.Run at 0x7f5a9c43e7d0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de10c0",
   "metadata": {},
   "source": [
    "One can check the results of the experiment run by accessing **run.result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44455fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_sparse_reward': 0.0, 'average_total_reward': 14.290098302224868}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run.result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "In practice, the reward should be much higher if optimized. Checkout the graph in the [README](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/human_aware_rl) in human_aware_rl module for baseline performances.\n",
    "\n",
    "Similarly, you can train BC agents with the [reproduce_bc.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/imitation/reproduce_bc.py) file under the human_aware_rl/imitation directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f493c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/osuke/cook_vanila/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle\n",
      "Number of trajectories processed for each layout: {'cramped_room': 14}\n",
      "Train on 28539 samples, validate on 5037 samples\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - tensorflow - OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28539/28539 - 0s - loss: 0.9498 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.8859 - val_sparse_categorical_accuracy: 0.7058 - lr: 0.0010 - 392ms/epoch - 14us/sample\n",
      "Epoch 2/10\n",
      "28539/28539 - 0s - loss: 0.8461 - sparse_categorical_accuracy: 0.7248 - val_loss: 0.8216 - val_sparse_categorical_accuracy: 0.7030 - lr: 0.0010 - 306ms/epoch - 11us/sample\n",
      "Epoch 3/10\n",
      "28539/28539 - 0s - loss: 0.8095 - sparse_categorical_accuracy: 0.7251 - val_loss: 0.8048 - val_sparse_categorical_accuracy: 0.7006 - lr: 0.0010 - 303ms/epoch - 11us/sample\n",
      "Epoch 4/10\n",
      "28539/28539 - 0s - loss: 0.7899 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.7878 - val_sparse_categorical_accuracy: 0.7058 - lr: 0.0010 - 304ms/epoch - 11us/sample\n",
      "Epoch 5/10\n",
      "28539/28539 - 0s - loss: 0.7778 - sparse_categorical_accuracy: 0.7229 - val_loss: 0.7812 - val_sparse_categorical_accuracy: 0.7058 - lr: 0.0010 - 303ms/epoch - 11us/sample\n",
      "Epoch 6/10\n",
      "28539/28539 - 0s - loss: 0.7705 - sparse_categorical_accuracy: 0.7234 - val_loss: 0.7748 - val_sparse_categorical_accuracy: 0.7052 - lr: 0.0010 - 300ms/epoch - 11us/sample\n",
      "Epoch 7/10\n",
      "28539/28539 - 0s - loss: 0.7650 - sparse_categorical_accuracy: 0.7241 - val_loss: 0.7683 - val_sparse_categorical_accuracy: 0.7014 - lr: 0.0010 - 303ms/epoch - 11us/sample\n",
      "Epoch 8/10\n",
      "28539/28539 - 0s - loss: 0.7603 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.7655 - val_sparse_categorical_accuracy: 0.7050 - lr: 0.0010 - 302ms/epoch - 11us/sample\n",
      "Epoch 9/10\n",
      "28539/28539 - 0s - loss: 0.7556 - sparse_categorical_accuracy: 0.7232 - val_loss: 0.7553 - val_sparse_categorical_accuracy: 0.6941 - lr: 0.0010 - 302ms/epoch - 11us/sample\n",
      "Epoch 10/10\n",
      "28539/28539 - 0s - loss: 0.7522 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.7546 - val_sparse_categorical_accuracy: 0.7004 - lr: 0.0010 - 306ms/epoch - 11us/sample\n",
      "Saving bc model at  path/to/bc_dir\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f5f6c099c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = \"cramped_room\" # any compatible layouts \n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import (\n",
    "    get_bc_params, # get the configuration for BC agents\n",
    "    train_bc_model, # train the BC model\n",
    ")\n",
    "from human_aware_rl.static import (\n",
    "    CLEAN_2019_HUMAN_DATA_TRAIN, # human trajectories\n",
    ")\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"path/to/bc_dir\", bc_params, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. To load an agent, you can use the load_agent function in the [rllib.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/rllib/rllib.py) file. For the purpose of demonstration, I will be loading a local agent, which is also one of the agents included in the web demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844332c3",
   "metadata": {},
   "source": [
    "## 1.1): Loading PPO agent\n",
    "The PPO agents are all trained via the Ray trainer, so to load a trained agent, we can just use the load_agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aeaae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:23:26,812\tWARNING deprecation.py:48 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7f5f840346d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent\n",
    "agent_path = \"src/overcooked_demo/server/static/assets/agents/RllibCrampedRoomSP/agent\"\n",
    "# The first argument is the path to the saved trainer, we then loads the agent associated with that trainner\n",
    "## If you use the experiment setup provided, the saved path should be the results_dir in the configuration\n",
    "# The second argument is the type of agent to load, which only matters if it is not a self-play agent \n",
    "# The third argument is the agent_index, which is not directly related to the training\n",
    "## It is used in creating the RllibAgent class that is used for evaluation\n",
    "ppo_agent = load_agent(agent_path,\"ppo\",0)\n",
    "ppo_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143edeb6",
   "metadata": {},
   "source": [
    "This function loads an agent from the trainer. The RllibAgent class is a wrapper around the core policy, which simplifies pairing and evaluating different type of agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.2) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94ab2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/osuke/miniconda3/envs/cook_vanila/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - tensorflow - From /home/osuke/miniconda3/envs/cook_vanila/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/osuke/miniconda3/envs/cook_vanila/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - tensorflow - From /home/osuke/miniconda3/envs/cook_vanila/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x7f5f91d10a10>,\n",
       " {'eager': True,\n",
       "  'use_lstm': False,\n",
       "  'cell_size': 256,\n",
       "  'data_params': {'layouts': ['cramped_room'],\n",
       "   'check_trajectories': False,\n",
       "   'featurize_states': True,\n",
       "   'data_path': '/home/osuke/cook_vanila/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle'},\n",
       "  'mdp_params': {'layout_name': 'cramped_room', 'old_dynamics': True},\n",
       "  'env_params': {'horizon': 400,\n",
       "   'mlam_params': {'start_orientations': False,\n",
       "    'wait_allowed': False,\n",
       "    'counter_goals': [],\n",
       "    'counter_drop': [],\n",
       "    'counter_pickup': [],\n",
       "    'same_motion_goals': True}},\n",
       "  'mdp_fn_params': {},\n",
       "  'mlp_params': {'num_layers': 2, 'net_arch': [64, 64]},\n",
       "  'training_params': {'epochs': 10,\n",
       "   'validation_split': 0.15,\n",
       "   'batch_size': 64,\n",
       "   'learning_rate': 0.001,\n",
       "   'use_class_weights': False},\n",
       "  'evaluation_params': {'ep_length': 400, 'num_games': 1, 'display': False},\n",
       "  'action_shape': (6,),\n",
       "  'observation_shape': (96,)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"path/to/bc_dir/\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c37a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7f5f4862f150>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(\n",
    "        bc_model, bc_params, stochastic=True\n",
    "    )\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent = RlLibAgent(bc_policy,0,base_env.featurize_state_mdp)\n",
    "bc_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c3f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:28:45,200\tWARNING deprecation.py:48 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7f5a886c7750>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent_pair\n",
    "# if we want to load a self-play agent\n",
    "ap_sp = load_agent_pair(agent_path,\"ppo\",\"ppo\")\n",
    "ap_sp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249778ce",
   "metadata": {},
   "source": [
    "This is convenient when the agents trained are not self-play agents. For example, if we have a PPO agent trained with a BC agent, we can load both as an agent pair at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78bb724",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/bc_dir/config.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_886979/2362974423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbc_agent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"path/to/bc_dir/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0map_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_agent_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc_agent_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ppo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0map_bc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cook_vanila/overcooked_ai/src/human_aware_rl/rllib/rllib.py\u001b[0m in \u001b[0;36mload_agent_pair\u001b[0;34m(save_path, policy_id_0, policy_id_1)\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0mID\u001b[0m \u001b[0mpolicy_id_0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpolicy_id_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrespectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_agent_pair_from_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cook_vanila/overcooked_ai/src/human_aware_rl/rllib/rllib.py\u001b[0m in \u001b[0;36mload_trainer\u001b[0;34m(save_path, true_num_workers)\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;31m# Read in params used to create trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0;31m# We use dill (instead of pickle) here because we must deserialize functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/bc_dir/config.pkl'"
     ]
    }
   ],
   "source": [
    "bc_agent_path = \"path/to/bc_dir/\"\n",
    "ap_bc = load_agent_pair(bc_agent_path,\"ppo\",\"bc\")\n",
    "ap_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7f5ab03214d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap = AgentPair(ppo_agent,bc_agent)\n",
    "ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95787dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.benchmarking.AgentEvaluator at 0x7f5a9c37f6d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93676beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 228.00 (std: 9.80, se: 3.10); avg len: 400.00; : 100%|██████████| 10/10 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ep_states': array([[<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a9c469c50>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f8417bd90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5ab0324950>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353750>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353450>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353150>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353590>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5ac80dda90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353250>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f6c083910>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f6c083b90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f6c081050>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f6c081290>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f48353050>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f6c0816d0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f485a77d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f485a7910>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5f485a7a50>],\n",
       "        ...,\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a881488d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a882d1190>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a88148a90>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a787f5c50>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a787f5e10>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a787f5b90>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a78781050>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a88148ed0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a787f5fd0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a78667a90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a78667b10>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a78667e90>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a885270d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a9c779cd0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a88530dd0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a784d79d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a784d7bd0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7f5a784d7dd0>]],\n",
       "       dtype=object),\n",
       " 'env_params': array([{'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1}],\n",
       "       dtype=object),\n",
       " 'metadatas': {},\n",
       " 'ep_dones': array([[False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True]], dtype=object),\n",
       " 'ep_rewards': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=object),\n",
       " 'ep_lengths': array([400, 400, 400, 400, 400, 400, 400, 400, 400, 400]),\n",
       " 'ep_returns': array([220, 240, 240, 220, 220, 220, 240, 220, 240, 220]),\n",
       " 'ep_actions': array([[((0, -1), 'interact'), ((0, -1), (1, 0)),\n",
       "         ('interact', 'interact'), ..., ((0, -1), 'interact'),\n",
       "         ('interact', 'interact'), ((1, 0), (0, 1))],\n",
       "        [((0, -1), 'interact'), ((-1, 0), (-1, 0)), ('interact', (1, 0)),\n",
       "         ..., ((0, -1), (1, 0)), ((1, 0), (-1, 0)), ('interact', (0, -1))],\n",
       "        [((0, -1), (0, 0)), ('interact', 'interact'), ((-1, 0), (-1, 0)),\n",
       "         ..., ((-1, 0), (1, 0)), ((-1, 0), (0, -1)), ((0, -1), (1, 0))],\n",
       "        ...,\n",
       "        [((1, 0), (1, 0)), ((-1, 0), (0, 0)), ((0, 1), 'interact'), ...,\n",
       "         ((0, 0), (0, 1)), ('interact', (1, 0)), ((0, 1), (0, 1))],\n",
       "        [((0, -1), 'interact'), ((-1, 0), (1, 0)), ((-1, 0), (1, 0)), ...,\n",
       "         ((0, 1), 'interact'), ('interact', 'interact'),\n",
       "         ((-1, 0), (0, 0))],\n",
       "        [((0, -1), (1, 0)), ((-1, 0), 'interact'), ((0, -1), (-1, 0)),\n",
       "         ..., ((1, 0), (-1, 0)), ((-1, 0), (-1, 0)), ((-1, 0), (-1, 0))]],\n",
       "       dtype=object),\n",
       " 'ep_infos': array([[{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05907974, 0.0312724 , 0.00532675, 0.7713628 , 0.04464762,\n",
       "                 0.08831069]], dtype=float32)}, {'action_probs': array([[0.18678126, 0.02202618, 0.23557003, 0.23477088, 0.20448603,\n",
       "                 0.11636563]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.08592098, 0.04419251, 0.01254764, 0.6580513 , 0.06675874,\n",
       "                 0.13252884]], dtype=float32)}, {'action_probs': array([[0.0672214 , 0.00837825, 0.07990283, 0.11575788, 0.25486216,\n",
       "                 0.4738775 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[2.7064225e-01, 1.9541461e-02, 4.9530948e-04, 5.8600851e-03,\n",
       "                 8.4246807e-02, 6.1921406e-01]], dtype=float32)}, {'action_probs': array([[0.00580614, 0.00859931, 0.02139725, 0.09684932, 0.0869929 ,\n",
       "                 0.7803551 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None}        ,\n",
       "         {'agent_infos': [{'action_probs': array([[1.62130296e-01, 8.17277208e-02, 4.78618895e-04, 1.53928613e-02,\n",
       "                 1.11077495e-01, 6.29193068e-01]], dtype=float32)}, {'action_probs': array([[0.04261797, 0.10619015, 0.21594214, 0.2100662 , 0.15012743,\n",
       "                 0.2750561 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [5, 0], 'phi_s': None, 'phi_s_prime': None}          ,\n",
       "         {'agent_infos': [{'action_probs': array([[1.8424079e-02, 1.7820941e-02, 8.8650542e-01, 7.0152187e-04,\n",
       "                 8.2999896e-03, 6.8247996e-02]], dtype=float32)}, {'action_probs': array([[0.0470042 , 0.04281951, 0.7915324 , 0.02357263, 0.03181135,\n",
       "                 0.06325986]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[7, 41, 48, 74, 109, 136, 141, 147, 165, 173, 209, 240, 257, 271, 305, 330, 335, 360, 369], [2, 8, 37, 44, 75, 105, 112, 143, 175, 181, 200, 208, 214, 240, 272, 299, 304, 336, 372, 397]], 'useful_onion_pickup': [[7, 41, 48, 74, 109, 136, 141, 147, 165, 173, 209, 240, 271, 305, 330, 335, 360, 369], [2, 8, 37, 44, 75, 105, 112, 143, 175, 181, 200, 208, 214, 240, 272, 304, 336, 372, 397]], 'onion_drop': [[149], [195]], 'useful_onion_drop': [[149], [195]], 'potting_onion': [[12, 44, 72, 77, 112, 139, 145, 171, 182, 214, 246, 268, 275, 308, 333, 338, 367, 372], [5, 14, 40, 48, 80, 109, 115, 147, 179, 206, 211, 237, 244, 278, 302, 310, 340, 375]], 'dish_pickup': [[17, 98, 198, 228, 296, 394], [64, 133, 150, 259, 326, 357]], 'useful_dish_pickup': [[17, 98, 198, 228, 296, 394], [64, 133, 150, 259, 326, 357]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[34, 101, 202, 234, 299, 398], [69, 136, 167, 266, 330, 360]], 'soup_delivery': [[37, 104, 205, 238, 302], [72, 139, 170, 269, 333, 363]], 'soup_drop': [[], []], 'optimal_onion_potting': [[12, 44, 72, 77, 112, 139, 145, 171, 182, 214, 246, 268, 275, 308, 333, 338, 367, 372], [5, 14, 40, 48, 80, 109, 115, 147, 179, 206, 211, 237, 244, 278, 302, 310, 340, 375]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[12, 44, 72, 77, 112, 139, 145, 171, 182, 214, 246, 268, 275, 308, 333, 338, 367, 372], [5, 14, 40, 48, 80, 109, 115, 147, 179, 206, 211, 237, 244, 278, 302, 310, 340, 375]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([100, 120]), 'cumulative_shaped_rewards_by_agent': array([102, 102])}, 'ep_sparse_r': 220, 'ep_shaped_r': 204, 'ep_sparse_r_by_agent': array([100, 120]), 'ep_shaped_r_by_agent': array([102, 102]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05907974, 0.0312724 , 0.00532675, 0.7713628 , 0.04464762,\n",
       "                 0.08831069]], dtype=float32)}, {'action_probs': array([[0.18678126, 0.02202618, 0.23557003, 0.23477088, 0.20448603,\n",
       "                 0.11636563]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0066767 , 0.0016865 , 0.00285086, 0.04593059, 0.06299352,\n",
       "                 0.87986183]], dtype=float32)}, {'action_probs': array([[0.0250644 , 0.01812042, 0.8863435 , 0.05953316, 0.00604076,\n",
       "                 0.00489793]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.30423078, 0.05429408, 0.17528346, 0.03780591, 0.07597999,\n",
       "                 0.3524058 ]], dtype=float32)}, {'action_probs': array([[0.12910794, 0.08703195, 0.54620874, 0.1423225 , 0.05587604,\n",
       "                 0.03945281]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.07095208, 0.04201254, 0.6495503 , 0.0725463 , 0.05476913,\n",
       "                 0.1101696 ]], dtype=float32)}, {'action_probs': array([[0.35064447, 0.13585137, 0.16252753, 0.15558946, 0.05974722,\n",
       "                 0.13564004]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.17585772, 0.02148636, 0.37806496, 0.03772037, 0.06708383,\n",
       "                 0.31978673]], dtype=float32)}, {'action_probs': array([[0.10950343, 0.10323212, 0.46532544, 0.11051193, 0.07387739,\n",
       "                 0.13754965]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[2, 7, 39, 69, 74, 102, 115, 124, 135, 161, 169, 198, 220, 230, 261, 285, 294, 327, 363, 399], [3, 29, 38, 60, 74, 105, 136, 166, 173, 197, 230, 244, 251, 260, 293, 321, 330, 338, 355, 360, 365, 372]], 'useful_onion_pickup': [[2, 7, 39, 69, 74, 102, 124, 135, 161, 169, 198, 220, 230, 261, 285, 294, 327, 363, 399], [3, 29, 38, 60, 74, 105, 136, 166, 197, 230, 251, 260, 293, 330, 360, 365]], 'onion_drop': [[123], [250, 345, 371]], 'useful_onion_drop': [[], [345, 371]], 'potting_onion': [[5, 10, 43, 72, 100, 105, 132, 138, 166, 172, 203, 226, 233, 265, 291, 301, 331, 368], [7, 36, 41, 68, 77, 108, 140, 169, 195, 200, 235, 258, 263, 296, 328, 334, 358, 363]], 'dish_pickup': [[26, 59, 178, 185, 250, 316, 352, 376], [93, 122, 156, 203, 281]], 'useful_dish_pickup': [[26, 59, 178, 250, 316, 352, 376], [93, 122, 156, 203, 281]], 'dish_drop': [[184], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[30, 63, 192, 255, 321, 355, 388], [97, 128, 160, 223, 285]], 'soup_delivery': [[33, 66, 195, 258, 324, 358, 391], [100, 131, 163, 226, 288]], 'soup_drop': [[], []], 'optimal_onion_potting': [[5, 10, 43, 72, 100, 105, 132, 138, 166, 172, 203, 226, 233, 265, 291, 301, 331, 368], [7, 36, 41, 68, 77, 108, 140, 169, 195, 200, 235, 258, 263, 296, 328, 334, 358, 363]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[5, 10, 43, 72, 100, 105, 132, 138, 166, 172, 203, 226, 233, 265, 291, 301, 331, 368], [7, 36, 41, 68, 77, 108, 140, 169, 195, 200, 235, 258, 263, 296, 328, 334, 358, 363]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([140, 100]), 'cumulative_shaped_rewards_by_agent': array([110,  94])}, 'ep_sparse_r': 240, 'ep_shaped_r': 204, 'ep_sparse_r_by_agent': array([140, 100]), 'ep_shaped_r_by_agent': array([110,  94]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05907974, 0.0312724 , 0.00532675, 0.7713628 , 0.04464762,\n",
       "                 0.08831069]], dtype=float32)}, {'action_probs': array([[0.18678126, 0.02202618, 0.23557003, 0.23477088, 0.20448603,\n",
       "                 0.11636563]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05907974, 0.0312724 , 0.00532675, 0.7713628 , 0.04464762,\n",
       "                 0.08831069]], dtype=float32)}, {'action_probs': array([[0.18678126, 0.02202618, 0.23557003, 0.23477088, 0.20448603,\n",
       "                 0.11636563]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.22520113, 0.15676036, 0.07125456, 0.22195785, 0.11520489,\n",
       "                 0.20962116]], dtype=float32)}, {'action_probs': array([[0.17911202, 0.08941687, 0.34012595, 0.17718472, 0.08446063,\n",
       "                 0.12969992]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.37659782, 0.2531673 , 0.0399803 , 0.09618165, 0.06895013,\n",
       "                 0.16512278]], dtype=float32)}, {'action_probs': array([[0.07007125, 0.06786177, 0.05510295, 0.12693143, 0.08990484,\n",
       "                 0.59012777]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.30516103, 0.27089655, 0.045586  , 0.1629476 , 0.07557373,\n",
       "                 0.139835  ]], dtype=float32)}, {'action_probs': array([[0.11077918, 0.12157034, 0.22351897, 0.38750198, 0.06219225,\n",
       "                 0.09443727]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[3, 8, 32, 37, 72, 103, 127, 133, 158, 166, 200, 207, 229, 260, 285, 293, 327, 351, 360, 386, 390], [4, 40, 57, 71, 96, 101, 134, 167, 191, 197, 230, 256, 262, 296, 302, 326, 359]], 'useful_onion_pickup': [[3, 8, 32, 37, 72, 103, 127, 133, 158, 166, 200, 229, 260, 285, 293, 327, 351, 360, 386, 390], [4, 40, 57, 71, 101, 134, 167, 197, 230, 256, 262, 296, 326, 359]], 'onion_drop': [[388], []], 'useful_onion_drop': [[], []], 'potting_onion': [[6, 12, 35, 41, 76, 107, 130, 137, 164, 169, 203, 227, 232, 263, 291, 296, 332, 357, 367, 393], [9, 44, 68, 74, 99, 105, 141, 171, 195, 200, 234, 260, 266, 299, 323, 330, 364]], 'dish_pickup': [[48, 93, 173, 188, 251, 305], [28, 124, 157, 218, 284, 345, 378]], 'useful_dish_pickup': [[48, 93, 173, 251, 305], [28, 124, 157, 218, 284, 345, 378]], 'dish_drop': [[187], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[64, 96, 191, 254, 320], [32, 127, 161, 224, 287, 352, 390]], 'soup_delivery': [[67, 99, 194, 257, 323], [35, 130, 164, 227, 290, 355, 394]], 'soup_drop': [[], []], 'optimal_onion_potting': [[6, 12, 35, 41, 76, 107, 130, 137, 164, 169, 203, 227, 232, 263, 291, 296, 332, 357, 367, 393], [9, 44, 68, 74, 99, 105, 141, 171, 195, 200, 234, 260, 266, 299, 323, 330, 364]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[6, 12, 35, 41, 76, 107, 130, 137, 164, 169, 203, 227, 232, 263, 291, 296, 332, 357, 367, 393], [9, 44, 68, 74, 99, 105, 141, 171, 195, 200, 234, 260, 266, 299, 323, 330, 364]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([100, 140]), 'cumulative_shaped_rewards_by_agent': array([100, 107])}, 'ep_sparse_r': 240, 'ep_shaped_r': 207, 'ep_sparse_r_by_agent': array([100, 140]), 'ep_shaped_r_by_agent': array([100, 107]), 'ep_length': 400}}],\n",
       "        ...,\n",
       "        [{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.257384  , 0.07242557, 0.03361128, 0.334402  , 0.11708697,\n",
       "                 0.18509017]], dtype=float32)}, {'action_probs': array([[0.05233457, 0.020369  , 0.15200928, 0.02360934, 0.12415903,\n",
       "                 0.62751883]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.711044  , 0.1835919 , 0.0111485 , 0.03965871, 0.03748894,\n",
       "                 0.01706809]], dtype=float32)}, {'action_probs': array([[0.04013383, 0.02558571, 0.03905554, 0.15112458, 0.19865419,\n",
       "                 0.54544616]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.09610928, 0.19332224, 0.09533696, 0.23096867, 0.12968108,\n",
       "                 0.25458178]], dtype=float32)}, {'action_probs': array([[0.19207376, 0.17527176, 0.14534324, 0.1022956 , 0.10377271,\n",
       "                 0.28124294]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.08967404, 0.19163252, 0.10850222, 0.19744407, 0.13641723,\n",
       "                 0.27632996]], dtype=float32)}, {'action_probs': array([[0.19094013, 0.21263202, 0.18384439, 0.15463015, 0.09693623,\n",
       "                 0.16101712]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.10065805, 0.1466062 , 0.11284804, 0.2275879 , 0.15141164,\n",
       "                 0.26088816]], dtype=float32)}, {'action_probs': array([[0.13373509, 0.15954591, 0.19453305, 0.16474979, 0.08969126,\n",
       "                 0.2577449 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[7, 39, 73, 82, 107, 112, 139, 163, 169, 176, 203, 227, 234, 266, 273, 295, 304, 338, 357, 382, 389], [2, 10, 22, 38, 64, 78, 108, 128, 139, 170, 205, 235, 267, 304, 327, 335]], 'useful_onion_pickup': [[7, 39, 73, 107, 112, 139, 163, 169, 176, 203, 227, 234, 266, 273, 295, 304, 338, 357, 382, 389], [2, 10, 38, 78, 108, 139, 170, 205, 235, 267, 304, 327, 335]], 'onion_drop': [[114], []], 'useful_onion_drop': [[114], []], 'potting_onion': [[10, 43, 78, 105, 110, 144, 167, 174, 200, 206, 232, 237, 270, 276, 302, 308, 342, 374, 387, 392], [5, 13, 36, 41, 73, 81, 112, 137, 142, 177, 209, 239, 273, 310, 333, 339]], 'dish_pickup': [[24, 59, 130, 311], [84, 94, 160, 181, 225, 243, 252, 294, 346]], 'useful_dish_pickup': [[24, 59, 130, 311], [84, 160, 181, 225, 243, 294, 346]], 'dish_drop': [[], [93, 246]], 'useful_dish_drop': [[], []], 'soup_pickup': [[33, 64, 133, 330], [101, 164, 197, 229, 259, 297, 371]], 'soup_delivery': [[36, 69, 137, 333], [104, 167, 200, 232, 263, 300, 374]], 'soup_drop': [[], []], 'optimal_onion_potting': [[10, 43, 78, 105, 110, 144, 167, 174, 200, 206, 232, 237, 270, 276, 302, 308, 342, 374, 387, 392], [5, 13, 36, 41, 73, 81, 112, 137, 142, 177, 209, 239, 273, 310, 333, 339]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[10, 43, 78, 105, 110, 144, 167, 174, 200, 206, 232, 237, 270, 276, 302, 308, 342, 374, 387, 392], [5, 13, 36, 41, 73, 81, 112, 137, 142, 177, 209, 239, 273, 310, 333, 339]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([ 80, 140]), 'cumulative_shaped_rewards_by_agent': array([ 92, 104])}, 'ep_sparse_r': 220, 'ep_shaped_r': 196, 'ep_sparse_r_by_agent': array([ 80, 140]), 'ep_shaped_r_by_agent': array([ 92, 104]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05907974, 0.0312724 , 0.00532675, 0.7713628 , 0.04464762,\n",
       "                 0.08831069]], dtype=float32)}, {'action_probs': array([[0.18678126, 0.02202618, 0.23557003, 0.23477088, 0.20448603,\n",
       "                 0.11636563]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.00511688, 0.00283186, 0.00165085, 0.02412008, 0.03862156,\n",
       "                 0.9276588 ]], dtype=float32)}, {'action_probs': array([[0.03749165, 0.00450588, 0.48071298, 0.08066729, 0.17251569,\n",
       "                 0.22410654]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.1351163 , 0.38967142, 0.07290639, 0.13141371, 0.10973662,\n",
       "                 0.16115557]], dtype=float32)}, {'action_probs': array([[0.08496383, 0.2569139 , 0.0560245 , 0.12961966, 0.13940455,\n",
       "                 0.33307356]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.07710923, 0.24698864, 0.04525313, 0.19210327, 0.12827109,\n",
       "                 0.31027463]], dtype=float32)}, {'action_probs': array([[0.10957852, 0.3208015 , 0.04854194, 0.10719448, 0.12240278,\n",
       "                 0.29148078]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.07695852, 0.24687365, 0.04603788, 0.20249379, 0.12494335,\n",
       "                 0.30269286]], dtype=float32)}, {'action_probs': array([[0.10788538, 0.33762565, 0.0462497 , 0.10713982, 0.12040024,\n",
       "                 0.28069922]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[3, 8, 38, 70, 101, 135, 142, 164, 189, 195, 214, 227, 261, 284, 290, 321, 329, 342, 355, 380, 388, 393], [4, 32, 37, 64, 69, 74, 100, 106, 132, 165, 196, 228, 252, 258, 291, 315, 327, 356, 393]], 'useful_onion_pickup': [[3, 8, 38, 70, 101, 135, 164, 189, 195, 227, 261, 284, 290, 321, 329, 355, 380, 388, 393], [4, 37, 69, 74, 100, 106, 132, 165, 196, 228, 252, 258, 291, 327, 356, 393]], 'onion_drop': [[333], []], 'useful_onion_drop': [[333], []], 'potting_onion': [[6, 11, 44, 74, 106, 138, 161, 167, 193, 198, 225, 230, 264, 287, 293, 327, 353, 358, 386, 391, 396], [8, 35, 42, 67, 72, 98, 104, 130, 135, 169, 200, 232, 256, 261, 295, 325, 330, 360]], 'dish_pickup': [[28, 61, 88, 120, 238, 307], [150, 186, 214, 280, 345, 377]], 'useful_dish_pickup': [[28, 61, 88, 120, 238, 307], [150, 186, 214, 280, 345, 377]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[32, 64, 94, 127, 253, 315], [158, 189, 220, 284, 350, 381]], 'soup_delivery': [[35, 67, 97, 130, 256, 318], [162, 193, 223, 288, 353, 385]], 'soup_drop': [[], []], 'optimal_onion_potting': [[6, 11, 44, 74, 106, 138, 161, 167, 193, 198, 225, 230, 264, 287, 293, 327, 353, 358, 386, 391, 396], [8, 35, 42, 67, 72, 98, 104, 130, 135, 169, 200, 232, 256, 261, 295, 325, 330, 360]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[6, 11, 44, 74, 106, 138, 161, 167, 193, 198, 225, 230, 264, 287, 293, 327, 353, 358, 386, 391, 396], [8, 35, 42, 67, 72, 98, 104, 130, 135, 169, 200, 232, 256, 261, 295, 325, 330, 360]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([120, 120]), 'cumulative_shaped_rewards_by_agent': array([111, 102])}, 'ep_sparse_r': 240, 'ep_shaped_r': 213, 'ep_sparse_r_by_agent': array([120, 120]), 'ep_shaped_r_by_agent': array([111, 102]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "                 0.04883546]], dtype=float32)}, {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "                 0.14160152]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.08592098, 0.04419251, 0.01254764, 0.6580513 , 0.06675874,\n",
       "                 0.13252884]], dtype=float32)}, {'action_probs': array([[0.0672214 , 0.00837825, 0.07990283, 0.11575788, 0.25486216,\n",
       "                 0.4738775 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.04111456, 0.02660614, 0.00543395, 0.208779  , 0.14062755,\n",
       "                 0.5774388 ]], dtype=float32)}, {'action_probs': array([[0.0161925 , 0.00707728, 0.01645537, 0.9246853 , 0.02863953,\n",
       "                 0.00695   ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.12170789, 0.08644951, 0.18142617, 0.24429399, 0.12872307,\n",
       "                 0.23739934]], dtype=float32)}, {'action_probs': array([[0.113957  , 0.2852059 , 0.16882853, 0.20044366, 0.11662471,\n",
       "                 0.11494028]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0916115 , 0.15951157, 0.3434792 , 0.30932155, 0.0754429 ,\n",
       "                 0.02063328]], dtype=float32)}, {'action_probs': array([[0.12196653, 0.11409123, 0.2148795 , 0.3979223 , 0.10694352,\n",
       "                 0.04419688]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.2484984 , 0.12001882, 0.15213366, 0.20979688, 0.16769063,\n",
       "                 0.10186152]], dtype=float32)}, {'action_probs': array([[0.07054947, 0.03039164, 0.22480585, 0.39157158, 0.1142914 ,\n",
       "                 0.16839013]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[7, 39, 74, 104, 137, 168, 202, 224, 237, 261, 267, 298, 304, 333, 340], [1, 8, 30, 38, 65, 73, 96, 103, 128, 135, 160, 168, 196, 201, 234, 268, 277, 334, 346, 369, 375]], 'useful_onion_pickup': [[7, 39, 74, 104, 137, 168, 202, 224, 237, 261, 267, 298, 304, 333, 340], [1, 8, 30, 38, 73, 96, 103, 135, 160, 168, 201, 234, 268, 334, 369, 375]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[10, 43, 78, 108, 141, 175, 206, 230, 241, 265, 270, 302, 307, 338, 343], [4, 12, 36, 41, 71, 76, 101, 106, 132, 139, 164, 171, 199, 204, 237, 272, 299, 340, 367, 373, 384]], 'dish_pickup': [[18, 61, 94, 124, 157, 192, 282, 358, 377, 393], [208, 257, 310, 322]], 'useful_dish_pickup': [[18, 61, 94, 124, 157, 192, 282, 358, 377], [208, 257, 310]], 'dish_drop': [[392], [321]], 'useful_dish_drop': [[], []], 'soup_pickup': [[32, 65, 98, 128, 161, 196, 292, 363], [226, 261, 327]], 'soup_delivery': [[35, 69, 101, 131, 165, 199, 295, 367], [229, 264, 330]], 'soup_drop': [[], []], 'optimal_onion_potting': [[10, 43, 78, 108, 141, 175, 206, 230, 241, 265, 270, 302, 307, 338, 343], [4, 12, 36, 41, 71, 76, 101, 106, 132, 139, 164, 171, 199, 204, 237, 272, 299, 340, 367, 373, 384]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[10, 43, 78, 108, 141, 175, 206, 230, 241, 265, 270, 302, 307, 338, 343], [4, 12, 36, 41, 71, 76, 101, 106, 132, 139, 164, 171, 199, 204, 237, 272, 299, 340, 367, 373, 384]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([160,  60]), 'cumulative_shaped_rewards_by_agent': array([112,  87])}, 'ep_sparse_r': 220, 'ep_shaped_r': 199, 'ep_sparse_r_by_agent': array([160,  60]), 'ep_shaped_r_by_agent': array([112,  87]), 'ep_length': 400}}]],\n",
       "       dtype=object),\n",
       " 'mdp_params': array([{'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]}],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ap_sp: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "# 400: environment timestep horizon, \n",
    "## should not be necessary is the AgentEvaluator is created with a horizon, but good to have for clarity\n",
    "result = ae.evaluate_agent_pair(ap_sp,10,400)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fed7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220, 240, 240, 220, 220, 220, 240, 220, 240, 220])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"ep_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264f7842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 156.00 (std: 14.97, se: 4.73); avg len: 400.00; : 100%|█| 10/10 [00:12<\n"
     ]
    }
   ],
   "source": [
    "# we can use any AgentPair class, like the ap_bc object we created earlier\n",
    "# as we can see the performance is not as good as the self-play agents\n",
    "result = ae.evaluate_agent_pair(ap_bc,10,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464d0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 226.00 (std: 12.81, se: 4.05); avg len: 400.00; : 100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9238e63c5d9b4074ae1704ef67cb3b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "# here we use the self-play agentPair created earlier again\n",
    "trajs = ae.evaluate_agent_pair(ap_sp,10,400)\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f05f5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ep_states', 'env_params', 'metadatas', 'ep_dones', 'ep_rewards', 'ep_lengths', 'ep_returns', 'ep_actions', 'ep_infos', 'mdp_params'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d03a6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = [trajs[i][0] for i in trajs.keys() if type(trajs[i]) is not dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "805fe387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'players': [{'position': (1, 2), 'orientation': (0, -1), 'held_object': None},\n",
       "  {'position': (3, 1), 'orientation': (0, -1), 'held_object': None}],\n",
       " 'objects': [],\n",
       " 'bonus_orders': [],\n",
       " 'all_orders': [{'ingredients': ('onion', 'onion', 'onion')}],\n",
       " 'timestep': 0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[0][0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ef04fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee5dbd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, True], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e3737ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79d92520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 200)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[4], first[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b924a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([((0, -1), (-1, 0)), ((-1, 0), (1, 0)), ('interact', 'interact'),\n",
       "       ('interact', (1, 0)), ((1, 0), (0, -1)), ((0, -1), (0, 1)),\n",
       "       ('interact', 'interact'), ((0, -1), (-1, 0)), ((-1, 0), (0, -1)),\n",
       "       ((-1, 0), 'interact'), ('interact', (1, 0)), ((1, 0), (1, 0)),\n",
       "       ((0, -1), (0, -1)), ('interact', (0, 0)), ((0, 1), (0, -1)),\n",
       "       ((1, 0), (0, -1)), ('interact', (0, -1)), ((-1, 0), (1, 0)),\n",
       "       ((0, 0), (-1, 0)), ((0, 0), (0, 1)), ((0, -1), (0, 1)),\n",
       "       ((0, 1), 'interact'), ((-1, 0), (1, 0)), ((-1, 0), (-1, 0)),\n",
       "       ((0, -1), (-1, 0)), ((0, 1), (0, 0)), ('interact', (-1, 0)),\n",
       "       ((0, 0), (0, 0)), ((0, 0), (-1, 0)), ((0, 0), (0, 0)),\n",
       "       ((0, -1), 'interact'), ((1, 0), (0, -1)), ((0, -1), (-1, 0)),\n",
       "       ('interact', (1, 0)), ((0, -1), (1, 0)), ((1, 0), (1, 0)),\n",
       "       ((0, 1), (0, -1)), ('interact', 'interact'), ((0, -1), (-1, 0)),\n",
       "       ((-1, 0), 'interact'), ((1, 0), (1, 0)), ('interact', (0, -1)),\n",
       "       ((0, -1), 'interact'), ((0, 1), (-1, 0)), ((-1, 0), (0, 0)),\n",
       "       ((0, -1), (-1, 0)), ('interact', (0, 1)), ((1, 0), (0, -1)),\n",
       "       ((-1, 0), (0, 1)), ((-1, 0), (0, 1)), ((0, 0), (0, -1)),\n",
       "       ((0, 0), (1, 0)), ((0, -1), (0, 0)), ((1, 0), 'interact'),\n",
       "       ((1, 0), (0, 0)), ('interact', 'interact'), ((0, -1), (-1, 0)),\n",
       "       ((-1, 0), (-1, 0)), ((0, -1), (-1, 0)), ((-1, 0), (0, 1)),\n",
       "       ('interact', (0, 0)), ((0, 1), 'interact'), ((1, 0), 'interact'),\n",
       "       ((0, -1), (1, 0)), ((-1, 0), (0, -1)), ((1, 0), 'interact'),\n",
       "       ((0, 0), 'interact'), ((1, 0), (1, 0)), ((0, -1), (0, 1)),\n",
       "       ('interact', 'interact'), ((-1, 0), (0, -1)), ('interact', (1, 0)),\n",
       "       ((1, 0), 'interact'), ((0, -1), (0, -1)), ('interact', (-1, 0)),\n",
       "       ((-1, 0), (-1, 0)), ((0, 0), (0, -1)), ((-1, 0), 'interact'),\n",
       "       ((0, 1), (1, 0)), ('interact', (-1, 0)), ((0, -1), (0, 1)),\n",
       "       ('interact', 'interact'), ((1, 0), (0, -1)), ((-1, 0), 'interact'),\n",
       "       ('interact', (-1, 0)), ((0, 0), (0, 0)), ('interact', (-1, 0)),\n",
       "       ((0, -1), (-1, 0)), ((0, 0), (-1, 0)), ((1, 0), 'interact'),\n",
       "       ((0, 1), (0, -1)), ('interact', (0, 1)), ((-1, 0), (0, 0)),\n",
       "       ((-1, 0), 'interact'), ('interact', (0, 0)), ((0, -1), (0, 0)),\n",
       "       ((-1, 0), (1, 0)), ((-1, 0), (0, -1)), ('interact', (0, 0)),\n",
       "       ('interact', 'interact'), ((1, 0), (1, 0)), ((0, -1), (0, 1)),\n",
       "       ('interact', 'interact'), ((1, 0), (0, -1)), ('interact', (0, -1)),\n",
       "       ((1, 0), (1, 0)), ('interact', (1, 0)), ((0, 1), (1, 0)),\n",
       "       ('interact', (1, 0)), ((0, -1), 'interact'), ('interact', (0, 1)),\n",
       "       ((0, -1), (-1, 0)), ((-1, 0), (0, -1)), ('interact', 'interact'),\n",
       "       ((1, 0), (-1, 0)), ((0, 1), (0, 1)), ((1, 0), 'interact'),\n",
       "       ('interact', (0, 1)), ((0, 1), (0, -1)), ((0, -1), (0, 1)),\n",
       "       ((0, -1), (-1, 0)), ((0, 1), 'interact'), ((0, 0), (0, 1)),\n",
       "       ((-1, 0), (0, 1)), ((-1, 0), (0, 1)), ((-1, 0), (0, 1)),\n",
       "       ((-1, 0), (0, 1)), ((0, 0), 'interact'), ((-1, 0), (0, 1)),\n",
       "       ((1, 0), (0, -1)), ((0, -1), 'interact'), ((1, 0), (-1, 0)),\n",
       "       ((1, 0), (0, -1)), ((0, 1), 'interact'), ((-1, 0), (1, 0)),\n",
       "       ((0, 1), 'interact'), ((0, 1), (0, -1)), ((0, 0), 'interact'),\n",
       "       ((0, 1), (1, 0)), ((0, -1), (0, 1)), ('interact', 'interact'),\n",
       "       ((-1, 0), (0, 1)), ((0, 0), 'interact'), ('interact', (0, -1)),\n",
       "       ((1, 0), (1, 0)), ((0, -1), (1, 0)), ('interact', (0, 1)),\n",
       "       ((-1, 0), (0, -1)), ('interact', (1, 0)), ((1, 0), 'interact'),\n",
       "       ((0, -1), (0, -1)), ('interact', (0, 1)), ('interact', 'interact'),\n",
       "       ((0, -1), 'interact'), ((0, 1), 'interact'), ((0, 0), (0, 0)),\n",
       "       ((0, 1), (0, 1)), ((-1, 0), (0, 0)), ('interact', (0, -1)),\n",
       "       ((-1, 0), (0, 1)), ((0, 1), (0, 0)), ((-1, 0), (0, 0)),\n",
       "       ((0, 1), (0, 1)), ((0, 1), (1, 0)), ('interact', (0, 1)),\n",
       "       ((-1, 0), (1, 0)), ((0, -1), (0, 1)), ('interact', (0, 1)),\n",
       "       ((-1, 0), (0, 0)), ((1, 0), (0, -1)), ((0, -1), (0, 1)),\n",
       "       ('interact', (-1, 0)), ((1, 0), (0, -1)), ((0, 1), 'interact'),\n",
       "       ('interact', (-1, 0)), ((0, -1), 'interact'), ((1, 0), 'interact'),\n",
       "       ('interact', (1, 0)), ((0, 1), (0, -1)), ((-1, 0), (0, -1)),\n",
       "       ((0, 1), 'interact'), ((0, -1), 'interact'), ((0, -1), (-1, 0)),\n",
       "       ('interact', (0, 1)), ((0, 1), (0, 0)), ((-1, 0), 'interact'),\n",
       "       ((0, 1), (1, 0)), ((0, 1), (1, 0)), ('interact', (0, -1)),\n",
       "       ((0, 1), (0, -1)), ((0, 1), 'interact'), ((0, 0), (-1, 0)),\n",
       "       ((-1, 0), (0, -1)), ((0, 1), (0, 1)), ((-1, 0), (1, 0)),\n",
       "       ((1, 0), (0, 1)), ((-1, 0), (0, 0)), ((0, 1), 'interact'),\n",
       "       ('interact', (0, 0)), ((-1, 0), (0, -1)), ((0, 0), (0, 0)),\n",
       "       ((0, 0), 'interact'), ((0, 0), 'interact'),\n",
       "       ('interact', 'interact'), ((0, -1), (1, 0)), ((0, -1), (0, 1)),\n",
       "       ('interact', 'interact'), ((-1, 0), (-1, 0)),\n",
       "       ('interact', (-1, 0)), ((-1, 0), (0, 0)), ('interact', (-1, 0)),\n",
       "       ('interact', (0, -1)), ('interact', 'interact'), ((0, -1), (0, 1)),\n",
       "       ((-1, 0), (0, 1)), ((0, 0), (0, 1)), ((0, 1), (0, 1)),\n",
       "       ((-1, 0), (0, 1)), ('interact', (-1, 0)), ((-1, 0), (0, -1)),\n",
       "       ((0, 0), (0, 1)), ((-1, 0), (0, 1)), ('interact', (-1, 0)),\n",
       "       ((-1, 0), (0, 1)), ((-1, 0), (0, 1)), ('interact', (-1, 0)),\n",
       "       ((1, 0), (-1, 0)), ((1, 0), (0, -1)), ((-1, 0), (0, 0)),\n",
       "       ((0, -1), (-1, 0)), ((1, 0), 'interact'), ('interact', (-1, 0)),\n",
       "       ((1, 0), (0, 1)), ((0, 0), (1, 0)), ((0, 1), (0, 0)),\n",
       "       ((0, -1), (0, -1)), ((0, 1), 'interact'), ('interact', (-1, 0)),\n",
       "       ((-1, 0), 'interact'), ((1, 0), (1, 0)), ((1, 0), (0, -1)),\n",
       "       ('interact', 'interact'), ((0, -1), (1, 0)), ((-1, 0), (-1, 0)),\n",
       "       ((0, -1), 'interact'), ((1, 0), (1, 0)), ((1, 0), (0, -1)),\n",
       "       ('interact', 'interact'), ((1, 0), (0, 1)), ((1, 0), (-1, 0)),\n",
       "       ((0, -1), (-1, 0)), ('interact', 'interact'),\n",
       "       ('interact', 'interact'), ((0, 0), 'interact'),\n",
       "       ((-1, 0), 'interact'), ((1, 0), (0, 1)), ('interact', (0, 0)),\n",
       "       ((0, 1), (0, -1)), ((0, 1), (-1, 0)), ((0, 1), 'interact'),\n",
       "       ((-1, 0), (-1, 0)), ('interact', (-1, 0)), ('interact', (0, -1)),\n",
       "       ('interact', 'interact'), ((0, 1), (0, 1)), ((0, 1), 'interact'),\n",
       "       ((0, -1), (1, 0)), ((-1, 0), (0, 1)), ((0, 1), (0, -1)),\n",
       "       ((0, 1), 'interact'), ((1, 0), (1, 0)), ((0, -1), (0, 1)),\n",
       "       ((0, 1), 'interact'), ((0, -1), (0, 1)), ('interact', (0, 1)),\n",
       "       ((-1, 0), (-1, 0)), ((0, 0), (0, -1)), ('interact', (1, 0)),\n",
       "       ((1, 0), (1, 0)), ((0, 1), 'interact'), ((0, -1), (0, 1)),\n",
       "       ('interact', (-1, 0)), ((-1, 0), (0, -1)),\n",
       "       ('interact', 'interact'), ((1, 0), (0, 0)), ((0, 1), 'interact'),\n",
       "       ((0, 1), (-1, 0)), ((-1, 0), (1, 0)), ((1, 0), (-1, 0)),\n",
       "       ((1, 0), (1, 0)), ((0, 0), (0, 0)), ((0, 1), (1, 0)),\n",
       "       ('interact', (0, 1)), ((-1, 0), (0, 1)), ((0, 1), (1, 0)),\n",
       "       ((1, 0), (0, 0)), ((0, 1), (-1, 0)), ((0, 1), 'interact'),\n",
       "       ((0, 0), (-1, 0)), ((0, 1), (-1, 0)), ((0, 1), (-1, 0)),\n",
       "       ('interact', (1, 0)), ((-1, 0), (-1, 0)), ('interact', (0, -1)),\n",
       "       ((1, 0), (-1, 0)), ((0, -1), (0, 1)), ('interact', (0, 1)),\n",
       "       ((1, 0), (0, -1)), ((0, 1), (0, 0)), ('interact', 'interact'),\n",
       "       ((0, -1), (-1, 0)), ((1, 0), (0, -1)), ((0, 1), (1, 0)),\n",
       "       ((0, -1), (0, -1)), ((0, 0), 'interact'), ((0, -1), (-1, 0)),\n",
       "       ((1, 0), 'interact'), ('interact', (1, 0)), ((0, 1), (0, -1)),\n",
       "       ((-1, 0), (0, 0)), ((0, 1), 'interact'), ((0, -1), (1, 0)),\n",
       "       ('interact', 'interact'), ((0, 0), (0, 0)), ((0, 1), (0, 1)),\n",
       "       ((0, 1), (0, -1)), ((-1, 0), (0, 1)), ((0, 1), 'interact'),\n",
       "       ((0, 1), (0, 0)), ((0, 1), (0, -1)), ((0, 1), (1, 0)),\n",
       "       ((-1, 0), (-1, 0)), ((0, 0), (0, 0)), ((0, 0), (0, -1)),\n",
       "       ((0, 0), 'interact'), ('interact', (1, 0)), ((0, 1), (1, 0)),\n",
       "       ('interact', 'interact'), ((0, 1), (1, 0)), ((0, 1), (1, 0)),\n",
       "       ((1, 0), (0, 0)), ((0, 0), 'interact'), ((0, -1), (0, 0)),\n",
       "       ('interact', (0, 1)), ('interact', (-1, 0)), ((1, 0), (0, -1)),\n",
       "       ((0, 1), (-1, 0)), ('interact', (0, 0)), ((0, -1), (0, 0)),\n",
       "       ((1, 0), (0, 1)), ('interact', (0, -1)), ((0, 0), (1, 0)),\n",
       "       ((0, 1), 'interact'), ((0, 1), (0, -1)), ('interact', 'interact'),\n",
       "       ((-1, 0), (-1, 0)), ((0, -1), (-1, 0)), ((0, -1), (0, 0)),\n",
       "       ('interact', 'interact'), ((1, 0), (1, 0)), ((0, 0), (0, -1)),\n",
       "       ('interact', 'interact'), ((0, 1), (0, 0)),\n",
       "       ('interact', 'interact'), ((0, 0), (0, 0)), ((1, 0), (0, 1)),\n",
       "       ((0, 1), (-1, 0)), ((0, 1), (0, 1)), ((0, 1), 'interact'),\n",
       "       ((1, 0), (0, 1)), ((0, 1), (0, 0)), ((0, 1), (0, 1)),\n",
       "       ((0, 0), (0, 1)), ('interact', (-1, 0)), ((0, 1), (0, 1)),\n",
       "       ((0, 1), (0, -1)), ((-1, 0), (0, 0)), ('interact', (0, 1)),\n",
       "       ((1, 0), (0, 1)), ('interact', 'interact'), ((0, 1), 'interact'),\n",
       "       ((1, 0), (0, 1)), ((1, 0), (1, 0)), ((-1, 0), 'interact'),\n",
       "       ((-1, 0), (0, -1)), ('interact', (0, 0)), ('interact', 'interact'),\n",
       "       ((0, 1), (1, 0)), ((0, -1), (0, 1)), ('interact', 'interact'),\n",
       "       ((-1, 0), (0, -1)), ((0, 1), (1, 0)), ((-1, 0), 'interact'),\n",
       "       ((-1, 0), (-1, 0)), ((0, 1), (0, 0)), ((0, -1), (0, -1)),\n",
       "       ('interact', 'interact'), ((-1, 0), (0, 1)), ((-1, 0), (0, 1)),\n",
       "       ('interact', (0, 1))], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2ed07ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " {'agent_infos': [{'action_probs': array([[0.7036528 , 0.09754185, 0.01395884, 0.08778691, 0.04822406,\n",
       "            0.04883546]], dtype=float32)},\n",
       "   {'action_probs': array([[0.21080449, 0.04071233, 0.14455009, 0.22968942, 0.23264216,\n",
       "            0.14160152]], dtype=float32)}],\n",
       "  'sparse_r_by_agent': [0, 0],\n",
       "  'shaped_r_by_agent': [0, 0],\n",
       "  'phi_s': None,\n",
       "  'phi_s_prime': None})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first[7]), first[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93acb979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layout_name': 'cramped_room',\n",
       " 'terrain': [['X', 'X', 'P', 'X', 'X'],\n",
       "  ['O', ' ', ' ', ' ', 'O'],\n",
       "  ['X', ' ', ' ', ' ', 'X'],\n",
       "  ['X', 'D', 'X', 'S', 'X']],\n",
       " 'start_player_positions': [(1, 2), (3, 1)],\n",
       " 'start_bonus_orders': [],\n",
       " 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3,\n",
       "  'DISH_PICKUP_REWARD': 3,\n",
       "  'SOUP_PICKUP_REWARD': 5,\n",
       "  'DISH_DISP_DISTANCE_REW': 0,\n",
       "  'POT_DISTANCE_REW': 0,\n",
       "  'SOUP_DISTANCE_REW': 0},\n",
       " 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cook_vanila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
